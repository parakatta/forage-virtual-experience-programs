# -*- coding: utf-8 -*-
"""Copy of modeling_walkthrough.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Y6trmjYFBOSw8wfYiQf60DGYcrwfPgUh

# Task 3 - Modeling

This notebook will walk you through this task interactively, meaning that once you've imported this notebook into Google Colab, you'll be able to run individual cells of code independently, and see the results as you go.

To follow along, simply read the notes within the notebook and run the cells in order.

## Section 1 - Setup

First, we need to mount this notebook to our Google Drive folder, in order to access the CSV data file. If you haven't already, watch this video https://www.youtube.com/watch?v=woHxvbBLarQ to help you mount your Google Drive folder.
"""

import config.config as config
import pandas as pd
from preprocessing.data_management import load_dataset
import preprocessing.preprocessors as pp
import matplotlib.pyplot as plt
import numpy as np
import model.model as m

sales_df = load_dataset(config.DATASET_1)
stock_df = load_dataset(config.DATASET_2)
temp_df = load_dataset(config.DATASET_3)

sales_df = pp.convert_to_datetime(sales_df, config.TIME_COL)
stock_df = pp.convert_to_datetime(stock_df, config.TIME_COL)
temp_df = pp.convert_to_datetime(temp_df, config.TIME_COL)


sales_df = pp.convert_timestamp_to_hourly(sales_df, config.TIME_COL)
stock_df = pp.convert_timestamp_to_hourly(stock_df, config.TIME_COL)
temp_df = pp.convert_timestamp_to_hourly(temp_df, config.TIME_COL)


sales_agg = sales_df.groupby(['timestamp', 'product_id']).agg({'quantity': 'sum'}).reset_index()
stock_agg = stock_df.groupby(['timestamp', 'product_id']).agg({'estimated_stock_pct': 'mean'}).reset_index()
temp_agg = temp_df.groupby(['timestamp']).agg({'temperature': 'mean'}).reset_index()


merged_df = stock_agg.merge(sales_agg, on=['timestamp', 'product_id'], how='left')
merged_df = merged_df.merge(temp_agg, on='timestamp', how='left')
merged_df['quantity'] = merged_df['quantity'].fillna(0)


product_categories = sales_df[config.PRODUCT_CATEGORIES_COLS]
product_categories = product_categories.drop_duplicates()

product_price = sales_df[config.PRODUCT_PRICE_COLS]
product_price = product_price.drop_duplicates()

merged_df = merged_df.merge(product_categories, on="product_id", how="left")
merged_df = merged_df.merge(product_price, on="product_id", how="left")


merged_df = pp.convert_to_time(merged_df)
merged_df = pd.get_dummies(merged_df, columns=['category'])
merged_df.drop(columns=['product_id'], inplace=True)


X,y = m.split_data(merged_df)
accuracy, model = m.make_model(X,y)
print(f"Average MAE: {(sum(accuracy) / len(accuracy)):.2f}")

features = [i.split("__")[0] for i in X.columns]
importances = model.feature_importances_
indices = np.argsort(importances)

fig, ax = plt.subplots(figsize=(10, 20))
plt.title('Feature Importances')
plt.barh(range(len(indices)), importances[indices], color='b', align='center')
plt.yticks(range(len(indices)), [features[i] for i in indices])
plt.xlabel('Relative Importance')
plt.show()

